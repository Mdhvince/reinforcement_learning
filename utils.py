from itertools import count

import numpy as np


def update_sarsa(Q, sarsa_experience, gamma, lr, use_sarsamax=False):
    state, action, reward, next_state, next_action = sarsa_experience
    estimate = Q[state][action]

    if(use_sarsamax):
        # Q-learning
        # we switch to a greedy policy (so our target will use the Q_next generated by another policy) : Off policy learning
        Qsa_next = np.max(Q[next_state]) if next_state is not None else 0
    else:
        # sarsa
        # Our target will use the same policy that has been used to interract with the env (no modification as contrary to above we modified by taking the max)
        Qsa_next = Q[next_state][next_action] if next_state is not None else 0
     
    target = reward + (gamma * Qsa_next)
    error = target - estimate
    estimate =  estimate + (lr * error)
    return estimate

def update_double_q(Q_to_update, other_Q, sarsa_experience, gamma, lr):
    state, action, reward, next_state, done = sarsa_experience
    estimate = Q_to_update[state][action]
    argmax_Q_to_update = np.argmax(Q_to_update[next_state])

    # we use the action Q_to_update - But from a state-value estimate done by other_Q
    target = reward + gamma * other_Q[next_state][argmax_Q_to_update] * (not done)
    error = target - estimate
    estimate = estimate + lr * error
    return estimate


def epsilon_greedy(state, Q, eps):
    if np.random.random() > eps:
        return np.argmax(Q[state])
    else:
        return np.random.randint(len(Q[state]))


def decay_schedule(init_value, min_value, decay_ratio, max_steps, log_start=-2, log_base=10):
    """Learning rate decay algorithm for learning"""
    decay_steps = int(max_steps * decay_ratio)
    rem_steps = max_steps - decay_steps

    values = np.logspace(log_start, 0, decay_steps, base=log_base, endpoint=True)[::-1]
    values = (values - values.min()) / (values.max() - values.min())
    values = (init_value - min_value) * values + min_value
    values = np.pad(values, (0, rem_steps), 'edge')
    return values


def generate_trajectory(Q, eps, env, max_steps=200):
    """Generate set of experience tuple for 1 episode"""
    done, trajectory = False, []
    
    while not done:
        state = env.reset()

        for t in count():  # equivalent of a while True : t+=1
            action = epsilon_greedy(state, Q, eps)
            next_state, reward, done, _ = env.step(action)

            experience = (state, action, reward, next_state, done)
            trajectory.append(experience)

            if done: break

            if t >= max_steps - 1: # No trajectory generated in the required window so we RETRY.
                trajectory = []
                break

            state = next_state
    return np.array(trajectory, np.object)